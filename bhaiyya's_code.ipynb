{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_block.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKSzVth0WaRlzS/G56vxw/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saraanwer15/Brain_Tumour/blob/main/bhaiyya's_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwIdjMM_5MY2"
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from skimage.io import imread\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import argparse\n",
        "from torch import Tensor"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa-VgBq5G5aw"
      },
      "source": [
        "class BrainSegmentationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_files):\n",
        "        self.image_files = image_files\n",
        "        self.mask_files = []\n",
        "        for filepath in self.image_files:\n",
        "                mask_path = filepath.split(\".\")[-2] + \"_mask.\" + filepath.split(\".\")[-1]\n",
        "                self.mask_files.append(mask_path)\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "      image_tensor = Tensor(imread(self.image_files[i]))\n",
        "      mask_tensor = Tensor(imread(self.mask_files[i], as_gray=True))\n",
        "      image_tensor = image_tensor.permute(2,0,1)*1/255.0\n",
        "      #mask_tensor = mask_tensor.permute(2,0,1)*1/255.0\n",
        "      tup = (image_tensor,mask_tensor)\n",
        "      return tup\n",
        "        \n",
        "    def __len__(self):\n",
        "      return len(self.image_files)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZijKjgHADmJ"
      },
      "source": [
        "path = [\"/content/drive/My Drive/archive/kaggle_3m/TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_2.tif\"]\n",
        "a= BrainSegmentationDataset(path)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjkYUJIcuz8t",
        "outputId": "ab8ba931-2212-4fca-a810-cc37875bc7a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "i=0\n",
        "for (dirpath, dirnames, filenames) in os.walk(\"/content/drive/My Drive/archive/kaggle_3m\"):\n",
        "            for filename in sorted(\n",
        "                filter(lambda f: \".tif\" in f, filenames),\n",
        "                key=lambda x: int(x.split(\".\")[-2].split(\"_\")[4]),):\n",
        "              i= i+1\n",
        "print(i)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9h21-4_iCPM"
      },
      "source": [
        "def split_random(image_files, p):\n",
        "  l1 = []\n",
        "  l2 = []\n",
        "  for i in range(0,len(image_files)):\n",
        "    if np.random.uniform()<p:\n",
        "      l1.append(image_files[i])     \n",
        "    else:\n",
        "      l2.append(image_files[i])\n",
        "  return l1,l2"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HUsE-8uTgeb"
      },
      "source": [
        "def datasets(image_files_train, image_files_valid):\n",
        "    train = BrainSegmentationDataset(\n",
        "        image_files=image_files_train,\n",
        "    )\n",
        "    valid = BrainSegmentationDataset(\n",
        "        image_files=image_files_valid\n",
        "    )\n",
        "    return train, valid"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zERcDHjOwfv_"
      },
      "source": [
        "def load_data(image_path):\n",
        "  train_ds, valid_ds = split_random(image_path, 0.8)\n",
        "  train_ds, valid_ds = datasets(train_ds, valid_ds)\n",
        "  train_ld = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "  valid_ld = DataLoader(valid_ds, batch_size=32)\n",
        "  return train_ld,valid_ld"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfyer4pJQStD",
        "outputId": "803b0209-2dc2-42ff-81c1-e2d7b0c259a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\")\n",
        "image_path = []\n",
        "i=0\n",
        "for (dirpath, dirnames, filenames) in os.walk(\"/content/drive/My Drive/archive/kaggle_3m\"):\n",
        "  for filename in sorted( filter(lambda f: \".tif\" in f, filenames), key=lambda x: int(x.split(\".\")[-2].split(\"_\")[4]),):\n",
        "    filepath = os.path.join(dirpath, filename)\n",
        "    if \"mask\" not in filename:\n",
        "      i= i+1\n",
        "      image_path.append(filepath)\n",
        "loader_train, loader_valid = load_data(image_path)\n",
        "loaders = {\"train\": loader_train, \"valid\": loader_valid}        \n",
        "print(i)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_C3l8jBlZIh",
        "outputId": "1127f5bd-cf02-4037-a244-a273c162c908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "for i,data in enumerate(loader_valid):\n",
        "  x,y=data\n",
        "  print(x.shape, \"abc\", y.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([32, 3, 256, 256]) abc torch.Size([32, 256, 256])\n",
            "torch.Size([28, 3, 256, 256]) abc torch.Size([28, 256, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKsy0qOt09tH",
        "outputId": "08daf43c-03f6-49ef-957f-1d2d1ac7875b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(loader_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYwN6Tf9Wcuv"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(\n",
        "            features * 16, features * 8, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose2d(\n",
        "            features * 8, features * 4, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose2d(\n",
        "            features * 4, features * 2, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose2d(\n",
        "            features * 2, features, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "        return torch.sigmoid(self.conv(dec1))\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\n",
        "                        name + \"conv1\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=in_channels,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\n",
        "                        name + \"conv2\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=features,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyoA7dWlzb_V",
        "outputId": "e89e583f-c082-4690-dcd7-c403a3a8cbf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "unet = UNet()\n",
        "unet.to(device)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (encoder1): Sequential(\n",
              "    (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc1relu1): ReLU(inplace=True)\n",
              "    (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc1relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder2): Sequential(\n",
              "    (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc2relu1): ReLU(inplace=True)\n",
              "    (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc2relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder3): Sequential(\n",
              "    (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc3relu1): ReLU(inplace=True)\n",
              "    (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc3relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder4): Sequential(\n",
              "    (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc4relu1): ReLU(inplace=True)\n",
              "    (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc4relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (bottleneck): Sequential(\n",
              "    (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bottleneckrelu1): ReLU(inplace=True)\n",
              "    (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bottleneckrelu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder4): Sequential(\n",
              "    (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec4relu1): ReLU(inplace=True)\n",
              "    (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec4relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder3): Sequential(\n",
              "    (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec3relu1): ReLU(inplace=True)\n",
              "    (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec3relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder2): Sequential(\n",
              "    (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec2relu1): ReLU(inplace=True)\n",
              "    (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec2relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder1): Sequential(\n",
              "    (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec1relu1): ReLU(inplace=True)\n",
              "    (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec1relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JCCSDsHYWjx"
      },
      "source": [
        "import torch.optim as optim\n",
        " y_pred = torch.reshape(y_pred,(y_pred.shape[0],-1))\n",
        "y_true = torch.reshape(y_true,(y_true.shape[0],-1))\n",
        "dsc_loss = torch.nn.CrossEntropyLoss\n",
        "best_validation_dsc = 0.0\n",
        "\n",
        "optimizer = optim.Adam(unet.parameters(), lr=0.0001)\n",
        "\n",
        "loss_train = []\n",
        "loss_valid = []\n",
        "\n",
        "step = 0"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OskmP_wyG3M7"
      },
      "source": [
        "def dsc_per_volume(validation_pred, validation_true, patient_slice_index):\n",
        "    dsc_list = []\n",
        "    num_slices = np.bincount([p[0] for p in patient_slice_index])\n",
        "    index = 0\n",
        "    for p in range(len(num_slices)):\n",
        "        y_pred = np.array(validation_pred[index : index + num_slices[p]])\n",
        "        y_true = np.array(validation_true[index : index + num_slices[p]])\n",
        "        dsc_list.append(dsc(y_pred, y_true))\n",
        "        index += num_slices[p]\n",
        "    return dsc_list"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juU7dd8wZKqb",
        "outputId": "096c69de-e0c3-4753-c156-6a2eb080584f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "for epoch in tqdm(range(10), total=10):\n",
        "        for phase in [\"train\", \"valid\"]:\n",
        "            if phase == \"train\":\n",
        "                unet.train()\n",
        "            else:\n",
        "                unet.eval()\n",
        "\n",
        "            validation_pred = []\n",
        "            validation_true = []\n",
        "\n",
        "            for i, data in enumerate(loaders[phase]):\n",
        "                if phase == \"train\":\n",
        "                    step += 1\n",
        "\n",
        "                x, y_true = data\n",
        "                x, y_true = x.to(device), y_true.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "                    y_pred = unet(x)\n",
        "\n",
        "                    loss = dsc_loss(y_pred, y_true)\n",
        "\n",
        "                    if phase == \"valid\":\n",
        "                        loss_valid.append(loss.item())\n",
        "                        y_pred_np = y_pred.detach().cpu().numpy()\n",
        "                        validation_pred.extend(\n",
        "                            [y_pred_np[s] for s in range(y_pred_np.shape[0])]\n",
        "                        )\n",
        "                        y_true_np = y_true.detach().cpu().numpy()\n",
        "                        validation_true.extend(\n",
        "                            [y_true_np[s] for s in range(y_true_np.shape[0])]\n",
        "                        )\n",
        "\n",
        "                    if phase == \"train\":\n",
        "                        loss_train.append(loss.item())\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "              print('Epoch [{}/{}] - Iter[{}/{}], MSE loss:{:.6f}'.format(\n",
        "                epoch + 1, num_epoch, i + 1,\n",
        "                len(train_loader.dataset) // 32, loss.item() / 32\n",
        "            ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 1/10 [00:54<08:08, 54.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 2/10 [01:49<07:16, 54.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|███       | 3/10 [02:45<06:24, 54.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 40%|████      | 4/10 [03:41<05:32, 55.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|█████     | 5/10 [04:38<04:38, 55.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}